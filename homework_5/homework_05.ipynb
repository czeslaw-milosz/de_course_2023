{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "765e8406-d24b-4f4f-a48e-fef190c04627",
   "metadata": {},
   "source": [
    "# Homework 05 (Lakehouse with delta lake)\n",
    "\n",
    "## Rados≈Çaw Jurczak\n",
    "\n",
    "-------------------------------------------------\n",
    "\n",
    "A docker network `de_network` is used, created by\n",
    "```{bash}\n",
    "docker network create de_network\n",
    "```\n",
    "\n",
    "Minio was run with the following command:\n",
    "```{bash}\n",
    "docker run -p 9000:9000 -p 9090:9090 --name minio --network=de_network -v ~/minio/data:/data -e \"MINIO_ROOT_USER=admin\" -e \"MINIO_ROOT_PASSWORD=adminadmin\" quay.io/minio/minio server /data --console-address \":9090\"\n",
    "```\n",
    "\n",
    "To run the code below, you'll need to create a minio bucket called `hw5`.\n",
    "\n",
    "The notebook was run inside docker, set up by\n",
    "```{bash}\n",
    "docker run \\\n",
    "-it -d --rm \\\n",
    "--network=de_network \\\n",
    "-p 10000:8888 -p 4041:4040 \\\n",
    "-v \"${PWD}\":/home/rj/data_engineering \\\n",
    "quay.io/jupyter/all-spark-notebook\n",
    "```\n",
    "\n",
    "---------------------------------------------------\n",
    "The data follows the example scheme outlined in the homework description: \n",
    " - bronze layer: 3 tables per day sales of a product. The sale consists of 3 columns: `user`, `product`, and `time`. Table for product: `name` and `price`. Table for user: `name` and `location`. In total, bronze level consists of 5 tables: `sale_day_1`, `sale_day_2`, `sale_day_3`, `users`, `products`;\n",
    " - silver layer: a single table containing all sales with user and product information (union three per-day tables and join with user and product);\n",
    " - golden layer: tables with reports: the sum of money per location and amount of sales per product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23c0b6e6-de3d-4893-9aa0-9e149d4e96c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: delta-spark in /opt/conda/lib/python3.11/site-packages (3.0.0)\n",
      "Requirement already satisfied: pyspark<3.6.0,>=3.5.0 in /usr/local/spark/python (from delta-spark) (3.5.0)\n",
      "Requirement already satisfied: importlib-metadata>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from delta-spark) (6.8.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.11/site-packages (from importlib-metadata>=1.0.0->delta-spark) (3.17.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.11/site-packages (from pyspark<3.6.0,>=3.5.0->delta-spark) (0.10.9.7)\n",
      "Requirement already satisfied: randomtimestamp in /opt/conda/lib/python3.11/site-packages (2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install delta-spark\n",
    "!pip install randomtimestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a140a2b-369c-4b54-9a95-be43fe6323dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import random \n",
    "from random import randint\n",
    "\n",
    "import randomtimestamp\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import StringType\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2769e27b-b3c4-409e-895c-aca1ce96cac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_conf = (\n",
    "    SparkConf()\n",
    "    .set(\"spark.jars.packages\", 'org.apache.hadoop:hadoop-client:3.3.4,org.apache.hadoop:hadoop-aws:3.3.4,io.delta:delta-spark_2.12:3.0.0')\n",
    "  \n",
    "    .set(\"spark.driver.memory\", \"6g\")\n",
    "\n",
    "    \n",
    "    .set(\"spark.hadoop.fs.s3a.endpoint\", \"minio:9000\")\n",
    "    .set(\"spark.hadoop.fs.s3a.access.key\", \"admin\")\n",
    "    .set(\"spark.hadoop.fs.s3a.secret.key\", \"adminadmin\" )\n",
    "    .set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \n",
    "    .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .set('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\n",
    "    .set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "\n",
    "    .set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \n",
    "    .set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\") # enable adding columns on merge\n",
    ")\n",
    "sc = SparkContext.getOrCreate(spark_conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71308e92-ac3a-45d7-8edc-3f5eefd6c67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadoop version = 3.3.4\n",
      "Spark version = 3.5.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Hadoop version = {spark._jvm.org.apache.hadoop.util.VersionInfo.getVersion()}\")\n",
    "print(f\"Spark version = {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2763ee9f-6176-46aa-959c-008cb86d57fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SALES = (50_000, 75_000, 30_000)\n",
    "N_USERS = 5000\n",
    "N_PRODUCTS = 50\n",
    "N_LOCATIONS = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fd8226-1496-40d7-b08e-f9105b5ebd5b",
   "metadata": {},
   "source": [
    "#### Generate user and product tables and push them to Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f64ecf56-993b-46db-bf01-3353f29a6d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|  name|   location|\n",
      "+------+-----------+\n",
      "|user_0| location_4|\n",
      "|user_1| location_4|\n",
      "|user_2| location_9|\n",
      "|user_3|location_11|\n",
      "|user_4| location_5|\n",
      "+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+------+\n",
      "|     name| price|\n",
      "+---------+------+\n",
      "|product_0|  61.1|\n",
      "|product_1| 774.5|\n",
      "|product_2|  72.6|\n",
      "|product_3|151.35|\n",
      "|product_4|374.54|\n",
      "+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users = [\n",
    "    (f\"user_{i}\",\n",
    "     f\"location_{random.randint(0, N_LOCATIONS)}\")\n",
    "    for i in range(N_USERS)\n",
    "]\n",
    "products = [\n",
    "    (f\"product_{i}\",\n",
    "     round(random.uniform(0.1, 1000.0), 2))\n",
    "    for i in range(N_PRODUCTS)\n",
    "]\n",
    "\n",
    "user_names = [f\"user_{i}\" for i in range(N_USERS)]\n",
    "product_names = [f\"product_{i}\" for i in range(N_PRODUCTS)]\n",
    "\n",
    "user_df = spark.createDataFrame(users, [\"name\", \"location\"])\n",
    "user_df.write.format(\"delta\").mode(\"overwrite\").save(\"s3a://hw5/user\")\n",
    "user_df.show(5)\n",
    "\n",
    "product_df = spark.createDataFrame(products, [\"name\", \"price\"])\n",
    "product_df.write.format(\"delta\").mode(\"overwrite\").save(\"s3a://hw5/product\")\n",
    "product_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7269d16-753c-4ac9-a5f6-e3b445d8c11f",
   "metadata": {},
   "source": [
    "#### Generate sales tables for 3 days and save them to Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06d8e473-4e6d-4eae-a240-c8639df2c99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for day, n in zip((1, 2, 3), N_SALES):\n",
    "    sales = [\n",
    "        (random.choice(user_names),\n",
    "         random.choice(product_names),\n",
    "         randomtimestamp.randomtimestamp(start=datetime.datetime.strptime(f\"2023-11-1{day}\", \"%Y-%m-%d\"), end=datetime.datetime.strptime(f\"2023-11-1{day+1}\", \"%Y-%m-%d\")))\n",
    "         for _ in range(n)\n",
    "    ]\n",
    "    sales_df = spark.createDataFrame(sales, [\"user\", \"product\", \"time\"])\n",
    "    sales_df.write.format(\"delta\").mode(\"overwrite\").save(f\"s3a://hw5/sale_day_{day}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633ac9c3-c059-4665-ad3e-7dae1225a3a4",
   "metadata": {},
   "source": [
    "#### Create silver layer table: all sales with user and product information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "553f457b-8501-499e-8d4c-816287054271",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_1_df = spark.read.format(\"delta\").load(\"s3a://hw5/sale_day_1\")\n",
    "sales_2_df = spark.read.format(\"delta\").load(\"s3a://hw5/sale_day_2\")\n",
    "sales_3_df = spark.read.format(\"delta\").load(\"s3a://hw5/sale_day_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60ddf62f-0a57-4bf5-8737-6d9b2816b229",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_1_df.createOrReplaceTempView(\"sale_day_1\")\n",
    "sales_2_df.createOrReplaceTempView(\"sale_day_2\")\n",
    "sales_3_df.createOrReplaceTempView(\"sale_day_3\")\n",
    "\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    (SELECT * FROM sale_day_1) UNION (SELECT * FROM sale_day_2) UNION (SELECT * FROM sale_day_3)\n",
    "    \"\"\"\n",
    ").write.format(\"delta\").mode(\"overwrite\").save(\"s3a://hw5/all_sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43d20559-e085-45da-bc29-03eafc341269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------------------+\n",
      "|     user|   product|               time|\n",
      "+---------+----------+-------------------+\n",
      "|user_4752|product_17|2023-11-11 22:07:29|\n",
      "|user_1734| product_9|2023-11-11 16:12:11|\n",
      "| user_122|product_13|2023-11-11 08:51:41|\n",
      "|user_4006|product_33|2023-11-11 07:22:05|\n",
      "|user_1249|product_46|2023-11-11 05:20:48|\n",
      "+---------+----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_sales_df = spark.read.format(\"delta\").load(\"s3a://hw5/all_sales\")\n",
    "all_sales_df.count()\n",
    "all_sales_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72c9a385-97b9-4cfe-ad15-1f311cab4ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sales_df.createOrReplaceTempView(\"all_sales\")\n",
    "user_df.createOrReplaceTempView(\"user\")\n",
    "product_df.createOrReplaceTempView(\"product\")\n",
    "\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT tmp.user, tmp.product, tmp.time, tmp.location, product.price \n",
    "    FROM \n",
    "        (SELECT all_sales.user, all_sales.product, all_sales.time, user.location \n",
    "        FROM all_sales JOIN user ON all_sales.user = user.name) tmp \n",
    "        JOIN product ON product = product.name\n",
    "    \"\"\"\n",
    ").write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"overwrite\").save(\"s3a://hw5/all_sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aab4f93a-06ca-4726-bbdf-a6f384c9cc8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------------------+----------+------+\n",
      "|    user|   product|               time|  location| price|\n",
      "+--------+----------+-------------------+----------+------+\n",
      "|user_833|product_18|2023-11-12 22:33:18|location_6|234.88|\n",
      "|user_833|product_49|2023-11-12 15:58:23|location_6|250.61|\n",
      "|user_833|product_10|2023-11-12 19:07:09|location_6|157.42|\n",
      "|user_833|product_24|2023-11-12 00:51:51|location_6|615.58|\n",
      "|user_833| product_5|2023-11-12 11:58:00|location_6|626.56|\n",
      "+--------+----------+-------------------+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_sales_df = spark.read.format(\"delta\").load(\"s3a://hw5/all_sales\")\n",
    "all_sales_df.count()\n",
    "all_sales_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d62508-9221-4eea-89c3-deb2b0ed6503",
   "metadata": {},
   "source": [
    "#### Generate golden layer report tables: \n",
    "- the sum of money per location;\n",
    "- the amount of sales per product.\n",
    "- the number of sales made per product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75037eac-32a6-40b5-bc9b-07eeec564697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|   location|money_total|\n",
      "+-----------+-----------+\n",
      "| location_8| 5275035.79|\n",
      "|location_13| 5011824.58|\n",
      "| location_7| 4952984.58|\n",
      "| location_5| 4759338.52|\n",
      "| location_3| 4757949.09|\n",
      "| location_9| 4748990.88|\n",
      "|location_12| 4716790.45|\n",
      "|location_15|  4702247.0|\n",
      "|location_14| 4642540.08|\n",
      "| location_4| 4638386.58|\n",
      "| location_0|  4580559.6|\n",
      "| location_2| 4522191.13|\n",
      "| location_6| 4520359.91|\n",
      "|location_10| 4383713.81|\n",
      "| location_1| 4331228.59|\n",
      "|location_11| 4322011.98|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "money_per_location_df = all_sales_df.groupby(\"location\").agg(f.round(f.sum(\"price\"), 2).alias(\"money_total\")).sort(\"money_total\", ascending=False)\n",
    "money_per_location_df.show()\n",
    "money_per_location_df.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"overwrite\").save(\"s3a://hw5/report_sales_per_location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d745517c-a6c7-497c-b1a0-d19cca07de1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|   product|sales_total|\n",
      "+----------+-----------+\n",
      "| product_6|  2908606.0|\n",
      "| product_9| 2884797.84|\n",
      "|product_25|  2879218.8|\n",
      "|product_31|  2851241.6|\n",
      "|product_33| 2785560.96|\n",
      "|product_35| 2749942.08|\n",
      "|product_17| 2738903.04|\n",
      "|product_42| 2667326.27|\n",
      "|product_22| 2635459.98|\n",
      "| product_8| 2543285.15|\n",
      "|product_21|  2427015.2|\n",
      "| product_1|  2386234.5|\n",
      "|product_27| 2334865.56|\n",
      "|product_19|  2311372.3|\n",
      "|product_11| 2155337.85|\n",
      "|product_12| 2060566.95|\n",
      "| product_5| 1922286.08|\n",
      "|product_24|  1877519.0|\n",
      "|product_46|  1862724.3|\n",
      "|product_38|  1857887.9|\n",
      "+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_per_location_df = all_sales_df.groupby(\"product\").agg(f.round(f.sum(\"price\"), 2).alias(\"sales_total\")).sort(\"sales_total\", ascending=False)\n",
    "sales_per_location_df.show()\n",
    "sales_per_location_df.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"overwrite\").save(\"s3a://hw5/report_sales_per_product\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b025a1e-e319-4688-b631-24320bc77e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|   product|sales_count|\n",
      "+----------+-----------+\n",
      "|product_28|       3175|\n",
      "|product_48|       3174|\n",
      "|product_36|       3169|\n",
      "|product_20|       3164|\n",
      "|product_42|       3163|\n",
      "|product_23|       3161|\n",
      "|product_25|       3156|\n",
      "|product_45|       3138|\n",
      "|product_34|       3131|\n",
      "|product_33|       3129|\n",
      "|product_21|       3128|\n",
      "| product_9|       3126|\n",
      "|product_16|       3125|\n",
      "| product_3|       3123|\n",
      "|product_15|       3119|\n",
      "|product_35|       3117|\n",
      "|product_37|       3116|\n",
      "|product_14|       3114|\n",
      "|product_13|       3112|\n",
      "| product_4|       3112|\n",
      "+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_count_per_location_df = all_sales_df.groupby(\"product\").count().withColumnRenamed(\"count\", \"sales_count\").sort(\"sales_count\", ascending=False)\n",
    "sales_count_per_location_df.show()\n",
    "sales_count_per_location_df.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"overwrite\").save(\"s3a://hw5/report_count_per_product\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
