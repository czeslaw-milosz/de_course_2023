{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "765e8406-d24b-4f4f-a48e-fef190c04627",
   "metadata": {},
   "source": [
    "# Homework 06 (Datahub)\n",
    "\n",
    "## Rados≈Çaw Jurczak\n",
    "\n",
    "-------------------------------------------------\n",
    "\n",
    "A docker network `de_network` is used, created by\n",
    "```{bash}\n",
    "docker network create de_network\n",
    "```\n",
    "\n",
    "Minio was run with the following command:\n",
    "```{bash}\n",
    "docker run -p 9000:9000 -p 9090:9090 --name minio --network=de_network -v ~/minio/data:/data -e \"MINIO_ROOT_USER=admin\" -e \"MINIO_ROOT_PASSWORD=adminadmin\" quay.io/minio/minio server /data --console-address \":9090\"\n",
    "```\n",
    "\n",
    "To succesfully run the code below, you'll need to create a minio bucket called `hw6`.\n",
    "\n",
    "The notebook was run inside docker, set up by\n",
    "```{bash}\n",
    "docker run \\\n",
    "-it -d --rm \\\n",
    "--network=de_network \\\n",
    "-p 10000:8888 -p 4041:4040 \\\n",
    "-v \"${PWD}\":/home/rj/data_engineering \\\n",
    "quay.io/jupyter/all-spark-notebook\n",
    "```\n",
    "\n",
    "---------------------------------------------------\n",
    "Two base datasets are created: \n",
    " - `user` table, with columns `user_id`, `location_id`, `first_name`, `last_name`, `age`;\n",
    " - `location` table, with columns `location_id`, `zip_code`, `city`, `city_size`;\n",
    "\n",
    "An additional table `user_location` is created by joining location info to `user` dataset.\n",
    "\n",
    "Finally, a report table `user_age_by_city` is created by aggregating `user_location` by city and calculating average user age in each group.\n",
    "\n",
    "---------------------------------------------------\n",
    "Screens from datahub are attached separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23c0b6e6-de3d-4893-9aa0-9e149d4e96c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: delta-spark in /opt/conda/lib/python3.11/site-packages (3.0.0)\n",
      "Requirement already satisfied: pyspark<3.6.0,>=3.5.0 in /usr/local/spark/python (from delta-spark) (3.5.0)\n",
      "Requirement already satisfied: importlib-metadata>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from delta-spark) (6.8.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.11/site-packages (from importlib-metadata>=1.0.0->delta-spark) (3.17.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.11/site-packages (from pyspark<3.6.0,>=3.5.0->delta-spark) (0.10.9.7)\n",
      "Requirement already satisfied: randomtimestamp in /opt/conda/lib/python3.11/site-packages (2.2)\n",
      "Requirement already satisfied: names in /opt/conda/lib/python3.11/site-packages (0.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install delta-spark\n",
    "!pip install randomtimestamp\n",
    "!pip install names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a140a2b-369c-4b54-9a95-be43fe6323dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import random \n",
    "from tqdm import tqdm\n",
    "\n",
    "import names\n",
    "import randomtimestamp\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import StringType\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2769e27b-b3c4-409e-895c-aca1ce96cac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_conf = (\n",
    "    SparkConf()\n",
    "    .set(\"spark.jars.packages\", 'org.apache.hadoop:hadoop-client:3.3.4,org.apache.hadoop:hadoop-aws:3.3.4,io.delta:delta-spark_2.12:3.0.0')\n",
    "  \n",
    "    .set(\"spark.driver.memory\", \"6g\")\n",
    "\n",
    "    \n",
    "    .set(\"spark.hadoop.fs.s3a.endpoint\", \"minio:9000\")\n",
    "    .set(\"spark.hadoop.fs.s3a.access.key\", \"admin\")\n",
    "    .set(\"spark.hadoop.fs.s3a.secret.key\", \"adminadmin\" )\n",
    "    .set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \n",
    "    .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .set('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\n",
    "    .set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "\n",
    "    .set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \n",
    "    .set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    # .set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\") # enable adding columns on merge\n",
    ")\n",
    "sc = SparkContext.getOrCreate(spark_conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71308e92-ac3a-45d7-8edc-3f5eefd6c67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadoop version = 3.3.4\n",
      "Spark version = 3.5.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Hadoop version = {spark._jvm.org.apache.hadoop.util.VersionInfo.getVersion()}\")\n",
    "print(f\"Spark version = {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2763ee9f-6176-46aa-959c-008cb86d57fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_USERS = 20_000\n",
    "N_LOCATIONS = 1_000\n",
    "N_CITIES = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fd8226-1496-40d7-b08e-f9105b5ebd5b",
   "metadata": {},
   "source": [
    "#### Generate user and location datasets and push them to delta lake on Minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f64ecf56-993b-46db-bf01-3353f29a6d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+----------+-----------+---+\n",
      "|user_id| location_id|first_name|  last_name|age|\n",
      "+-------+------------+----------+-----------+---+\n",
      "| user_0|location_372| Roosevelt|      Watts| 41|\n",
      "| user_1|location_461|     Velma|      Loper| 65|\n",
      "| user_2|location_873|      Mark|Zimmerebner| 61|\n",
      "| user_3|location_973|   Douglas|    Kirksey| 58|\n",
      "| user_4|location_893|     Maria|    Swilley| 40|\n",
      "+-------+------------+----------+-----------+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------+--------+-------+---------+\n",
      "|location_id|zip_code|   city|city_size|\n",
      "+-----------+--------+-------+---------+\n",
      "| location_0|  78-256|city_27|   medium|\n",
      "| location_1|  47-932| city_6|    large|\n",
      "| location_2|  16-330|city_21|    large|\n",
      "| location_3|  12-350| city_4|    small|\n",
      "| location_4|  42-704|city_13|    large|\n",
      "+-----------+--------+-------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users = [\n",
    "    (f\"user_{i}\",\n",
    "     f\"location_{random.randint(0, N_LOCATIONS-1)}\",\n",
    "     names.get_first_name(),\n",
    "     names.get_last_name(),\n",
    "     random.randint(18, 100),\n",
    "    )\n",
    "    for i in range(N_USERS)\n",
    "]\n",
    "\n",
    "city2size = {\n",
    "    f\"city_{i}\": random.choice((\"small\", \"medium\", \"large\"))\n",
    "    for i in range(N_CITIES)\n",
    "}\n",
    "city_names = list(city2size.keys())\n",
    "locations = []\n",
    "for i in range(N_LOCATIONS):\n",
    "    city = random.choice(city_names)\n",
    "    locations.append((\n",
    "        f\"location_{i}\",\n",
    "        f\"{random.randint(10, 99)}-{random.randint(100, 999)}\",\n",
    "        city,\n",
    "        city2size[city],\n",
    "))\n",
    "\n",
    "user_df = spark.createDataFrame(users, [\"user_id\", \"location_id\", \"first_name\", \"last_name\", \"age\"])\n",
    "user_df.write.format(\"delta\").mode(\"overwrite\").save(\"s3a://hw6/user\")\n",
    "user_df.show(5)\n",
    "\n",
    "location_df = spark.createDataFrame(locations, [\"location_id\", \"zip_code\", \"city\", \"city_size\"])\n",
    "location_df.write.format(\"delta\").mode(\"overwrite\").save(\"s3a://hw6/location\")\n",
    "location_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7269d16-753c-4ac9-a5f6-e3b445d8c11f",
   "metadata": {},
   "source": [
    "#### Join users with locations and store the result in Minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06d8e473-4e6d-4eae-a240-c8639df2c99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------+---------+---+--------+------+---------+\n",
      "|location_id|   user_id|first_name|last_name|age|zip_code|  city|city_size|\n",
      "+-----------+----------+----------+---------+---+--------+------+---------+\n",
      "| location_8|user_18918|    Donald|  Rickman| 48|  76-507|city_3|   medium|\n",
      "| location_8|user_17549|  Jennifer|    Perez| 95|  76-507|city_3|   medium|\n",
      "| location_8|user_13131|    Evelyn|   Newman| 73|  76-507|city_3|   medium|\n",
      "| location_8|user_12875|      John|    Yates| 54|  76-507|city_3|   medium|\n",
      "| location_8|user_12411|      Erma|   Berger| 59|  76-507|city_3|   medium|\n",
      "+-----------+----------+----------+---------+---+--------+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_location_df = user_df.join(\n",
    "    location_df, on=\"location_id\"\n",
    ")\n",
    "user_location_df.write.format(\"delta\").mode(\"overwrite\").save(\"s3a://hw6/user_location\")\n",
    "user_location_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633ac9c3-c059-4665-ad3e-7dae1225a3a4",
   "metadata": {},
   "source": [
    "#### Create a report table: average user age by city; store the result in Minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60ddf62f-0a57-4bf5-8737-6d9b2816b229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+\n",
      "|   city|average_user_age|\n",
      "+-------+----------------+\n",
      "|city_29|           60.67|\n",
      "|city_20|            60.0|\n",
      "|city_13|           59.99|\n",
      "| city_3|           59.99|\n",
      "|city_10|           59.89|\n",
      "+-------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_age_by_city_df = user_location_df.select(\n",
    "    \"city\", \"age\"\n",
    ").groupby(\"city\").agg(\n",
    "    f.round(f.avg(\"age\"), 2).alias(\"average_user_age\")\n",
    ").sort(\"average_user_age\", ascending=False)\n",
    "user_age_by_city_df.write.format(\"delta\").mode(\"overwrite\").save(\"s3a://hw6/user_age_by_city\")\n",
    "user_age_by_city_df.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
